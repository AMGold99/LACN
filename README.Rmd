---
title: "LACN 2022 Survey"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE,
  paged.print = FALSE,
  echo = FALSE,
  fig.path = "output/"
  )
  
  load("lacn.RData")
```

This document walks you through the structure, purpose, and output of this repository. All scripts can be found in the **code** directory. 

# File Structure

```{r file-structure}
cat('Contents of lacn directory \n')
list.files("~/piper analysis/lacn")

cat('\nFiles in code subdirectory\n')
list.files("~/piper analysis/lacn/code")

cat('\nFiles in data subdirectory\n')
list.files("~/piper analysis/lacn/data")
```


# Load Data

Script: **1_read_data.R**

The raw survey data ("lacn_2022.csv") resides in the data subdirectory of the lacn folder. The first several lines of this script load this data and remove several redundant rows, as well as performing some highly specific cleaning that will likely not be relevant in the future. 

Next, we specify the Gooogle Sheets spreadsheet we want to connect with (this will come in handy a bit later).

The next task is creating a reference lookup table for all the questions, their descriptions, and response descriptions. We will call this our "Response Key." First we create a somewhat messy version from the raw data (**response_key_messy**). Then we send it over to Sheets for some manual cleaning before bringing it back into R, now calling it **response_key**. This will be crucial for maintaining consistent references throughout analysis and visualization.

```{r response-key}
cat('Reference Lookup Table:\n')
head(response_key, 3)
```

Our final task in this initial section is creating a table of question types. Some LACN questions are single-response, some are multi-choice (more than one can be selected), some allow for a matrix of responses per college, some a continuous numeric input, and one a ordinal ranking. If we want to automate the cleaning and analysis of the survey questions, we need to be able to separate out the single-response questions from the matrix questions, etc. The **question_type** dataframe, built manually in Google Sheets and then imported into R.

Now we can move to analyzing each question on its own terms.

# Cleaning 

Script: **2_clean.R**

One of the challenges with this data is how *wide* it is. The raw survey dataframe contains 257 variables. If we want to speed up our analysis, we need to break the dataset up by question. That way, when we want to analyze Q8, we can simply handle the Q8 dataset without having to sift through the entire 257-column raw dataset.

**clean.R** loops through each question and extracts its columns from the master dataset, along with institution name and enrollment. We then deposit each of those question-specific dataframes (specified as **current_question** in the for loop) into a "list," which is an object capable of containing other objects (like dataframes) within it. Now, we have a nice portable object we can manipulate, explore, and use in later analysis. Without this list, we would have to repeat ourselves every time we wanted to extract a single question and analyze it.

Below is a representation of the structure of that list (called **question_list**). As you can see, **question_list**, represented by the top-most black rectangle, contains within it 

```{r}
lobstr::ref(question_list)
```

# Functions for Analysis

Script: **3_functions.R**

## Motivation
To get a sense for why the following custom functions are useful, let's inspect some of the raw data.

Below are several questions from the survey. 

```{r paged.print=FALSE}
cat('Question 2\n')
head(question_list$Q2)

cat('\nQuestion 4\n')
head(question_list$Q4)

cat('\nQuestion 5\n')
head(question_list$Q5)

cat('\nQuestion 6\n')
head(question_list$Q6)

cat('\nQuestion 7\n')
head(question_list$Q7)

```
A brief inspection of these questions (and the original survey format) reveals their widely varying structure. Question 2 allows for only one response per participant, while Question 4 allows for multiple responses. Question 5 requires each respondent to rank a list of items, Question 6 provides a matrix for the respondent to fill in, and Question 7 asks for an unbounded numeric input reflecting the number of FTE professional staff.

Here, we're faced with a choice. First, we could analyze each of the 23 questions individually. While that is the most attractive option up front, pursuing that path quickly presents a problem: we will end up duplicating work when one question has the same form as an earlier one and therefore can be analyzed using essentially the same method. Questions 2 and 3, for example, are both single-response items. If we analyzed them separately, we would probably just copy and paste the code from Question 2 in order to work with Question 3. 

Our second option, then, is to build some functions that can deal with each of these question types efficiently. There are only five types: single, multi, matrix, continuous, and ranking. That is a much more manageable problem.

The challenge with this approach is building the functions. Custom functions can be a bit intimidating at first, but what they lack in simplicity they make up for in speed.

## Functions

Script: **3_functions.R**

Note: the summarising method of the functions can be adjusted. Currently they return averages when relevant, but we can change this to, for example, sum with little difficulty.

### Helper Functions
In **lacn_functions.R**, we find a handful of functions. First up are two helper functions: *selectionFunction*, which returns a vector of questions (e.g., c("Q4","Q7","Q2"...)) that belong to a certain question type, like "matrix", and *keyFunction*, which returns a reference table for response labels. Don't worry too much about these; they just end up getting wrapped into the main functions below.

### Analysis Functions

The four main types of question each get their own function (single, multi, matrix, and continuous). *Ranking* does not, as there is only one such question; building a function would be more trouble than it's worth. 
Without getting into too much detail, here's how each of them works

#### Single Function and Multi Function

Both of these merely group and aggregate the number of times each response was selected, then add a variable for the relative frequency of each response (with the denominator being the total number of respondents).

```{r single multi, paged.print=FALSE}
cat('Original data:\n')
head(question_list$Q2)
cat('\nAggregated data:\n')
head(all_list$single$Q2)
```

#### Matrix Function

The matrix function pivots then unpivots each question so that the matrix format of the original survey is recovered. It then summarises the responses in each cell.

```{r}
cat('Original data:\n')
head(question_list$Q6)

cat('\nAggregated data:\n')
head(all_list$matrix$Q6)


```


#### Continuous Function

The continuous function pivots and aggregates data by response to return some statistic on each response category.
```{r}
cat('Original data:\n')
head(question_list$Q7)

cat('\nAggregated data:\n')
head(all_list$continuous$Q7)

```
#### Ranking Analysis

The ranking question (Q5) doesn't get its own function. Here's how the analysis works (see **analysis.R**). We simply compute the desired statistic for the ranking of each "priority" (student engagment, first destination data) and then pivot the resultant dataset in anticipation of visualization.

#### Analyze Function

Finally, the analyze function allows us to do all of the above analysis for each question type in just a few lines of code (see next section).

#### Visualization Functions

* matrixPlot plots matrix questions on a barchart
* singlePlot plots single-answer questions on an averaged barchart
* tableViz produces the N/Mean/Median/Max/Min/[college] summary tables
* nTab produces rendered text output for stand-alone statements like "N = 35" * serviceTab creates service/program tally tables 
* serviceCustom adds a customized school-level column to a service/program dataframe in preparation for producing a custom serviceTab table. See use cases in **custom_template.Rmd** by searching "serviceCustom"


# Analysis

Script: **4_analysis.R**

Next, we apply the functions we built to the original data we have stored in **question_list**. The analyzeFunction that we built above allows us analyze any set of questions. That is, if we choose only the "single" questions, it will apply the singleFunction to them.

The map function (see [purrr](https://purrr.tidyverse.org/) for more) allows us to do exactly this. Let me explain the code below:
```{r}
all_questions <- unique(question_type$q_type)[c(2,3,5,6)]
cat('all_questions:\n')
all_questions
```

```r
all_list <- map(all_questions,
                ~ analyzeFunction(
                  .x
                )
)
```

The first argument in the map function is the **all_questions** vector. The second argument is the function we want to apply to each element in that vector (the '.x' is just a placeholder to represent each element in the vector). So first it will apply the singleFunction to the single questions, then the multiFunction to the multi questions, and so on, storing each question type separately in the list we're calling **all_list**. After this code, the **analysis.R** script goes on to add the ranking question into **all_list** separately, as it did not have a function.


## What did we create?
We now have a list containing the cleaned and summarised data for each type of question (single, multi, matrix, continuous, ranking). See the structure map below to get a sense of how the final data is stored. It can look a bit chaotic at first, but the basic idea is this: the master list contains different question types, which each contain the relevant questions, which each contain the actual variables and data for each summarised and aggregated response.

```{r list structure}
lobstr::ref(all_list)
```

Let's say you wanted to investigate conference attendance rates (Q9). You would note that Q9 is a multi-response question:

```{r q9 type, echo=TRUE}
question_type |> dplyr::filter(unique=="Q9")
```
Next, you would key into the master list in the following order: master list --> question type --> question. The '$' in the code below are how R digs into a deeper level of some object, like a list or a dataframe. Think of it as opening a door into an inner room of a house.

```{r list explore, echo=TRUE}
all_list$multi$Q9
```

# Report Building

## R Markdown and html rendering

The "docs" directory within "lacn" contains a processed copy of everything in the main directory. (Quick sidebar: Consider playing around with deleting the files in the parent "lacn" directory and only working in the docs directory. I was too nervous to experiment, but I think that could save you a few headaches; often, I would knit **index.Rmd** and find I had been editing the "docs" version of my scripts in the "code" directory, instead of the parent directory version, which is where my markdown documents "look" to find their source code.)

There are two key processes: 1) rerunning all scripts that make up the "project" each time you edit, in order to make your markdown documents aware of the updates; and 2) knitting **index.Rmd** to update the "docs" directory. If you don't knit **index.Rmd**, your updated code will not get copied to the "docs" directory.

1) In lacn/code, there is a file called **source.R**. This script "sources" (or runs) all the scripts in the code directory, then saves the results as something called a workspace image, which is just a special file that contains all the objects, datasets, and functions that were run in a given session. By saving this workspace image, you can now port it around anywhere, and make sure that any other document utilizing that code will always have a consistent, up-to-date version. To see how this works, take a moment to inspect **GeneralReport.Rmd**. In the "setup" chunk toward the top of the document:


load("../lacn.RData")

The two dots indicate that the "load" command should look in the relative parent directory (in this case, "lacn", since the report is stored in "docs"). This command loads the workspace image that we just ran with **source.R**. In this way, any change we make to the code will be acknowledged by the R Markdown file when we knit it.

To run **source.R**, either click the "source" buttton in the top right of the pane, or open the shell (top menu: *Tools* --> **Shell**) and run:

RScript code/source.R

2) Running **index.Rmd** copies all the code in your parent directory ("lacn") into the output directory, which we've specified as "docs" in _site.yml. This is an important step in order to keep "docs" in sync with "lacn." But again, play around with only storing your code in "docs." This will probably be much faster and less error-prone. 

Finally, knitting! Each time you've finished making changes to, say, **GeneralReport.Rmd**, click the "Knit" button in the top of the pane. This will process the markdown document and create a formatted html file. This is the file that should be referenced in _site.yml when specifying the contents of the website's pages.

Once you're finished knitting, you can then commit your changes using the following commands in the shell. (Top menu: *Tools* --> *Shell*).

git add <either put file names here, or just "." to add stage every modified file>

git commit -m "message describing the changes made"

git push 

The push command will prompt you to provide GitHub personal access token credentials. Visit [this documentation](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token) for more on how to create a GitHub PAT when connecting your RStudio project to a remote GitHub repository.


## Webpage hosting

This template is ready for web hosting through GitHub Pages, as it contains _site.yml, **index.html**, and the output directory "docs," which the web hosting software interprets as the place to find all the files you reference in _site.yml.

Each time you push a batch of commits to the remote GitHub repository connected to Pages, GitHub will automatically deploy all your changes. This typically takes 1-3 minutes. The small green checkmark on header of the GitHub repository will temporarily turn into a yellow circle while its processing and deploying the changes.

To adjust website settings, access your GitHub repository, then go to "Settings." Under the "Code and Automation" sidebar, click on "Pages." Under "Source," make sure that your site is being built from the same directory as the **output_dir** specified in _site.yml. 

Since we've set a Bootstrap theme, you can ignore the "Theme Chooser" section.

